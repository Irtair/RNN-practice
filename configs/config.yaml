data:
  data_dir: "./data"
  raw_path: "./data/tweetes.txt"
  processed_path: "./data/dataset_processed.txt"
  tokenizer_file_name: "wordlevel.json"
  tokenized_file_name: "tokenized.pt"
  model_save_dir: "models"
  train_batch_size: 320
  val_batch_size: 128
  test_batch_size: 128

model:
  best_model_file: "best_model.pt"
  embedding_dim: 256
  hidden_dim: 256
  num_layers: 2
  dropout: 0.3

training:
  epochs: 30
  learning_rate: 1e-3
  early_stopping:
    patience: 3