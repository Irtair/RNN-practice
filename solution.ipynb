{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457c35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c922548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "path = \"./configs/config.yaml\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b99648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞/–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "--–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import process_data\n",
    "\n",
    "cleaned = process_data(config[\"data\"][\"data_dir\"], config[\"data\"][\"raw_path\"], config[\"data\"][\"processed_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2ac0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä —É–∂–µ –ø—Ä–æ—à–µ–ª —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É\n"
     ]
    }
   ],
   "source": [
    "from src.tokenizer import train_tokenizer\n",
    "\n",
    "train_tokenizer(config[\"data\"][\"processed_path\"], config[\"data\"][\"tokenizer_file_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba7b1bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 1596901 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import data_tokenization\n",
    "\n",
    "tokenized = data_tokenization(config[\"data\"][\"data_dir\"], config[\"data\"][\"tokenized_file_name\"], cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8930fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train: 1277520, Val: 159690, Test: 159691\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import data_split\n",
    "\n",
    "train_seqs, val_seqs, test_seqs = data_split(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f2814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import TweeterDataset\n",
    "\n",
    "train_dataset = TweeterDataset(train_seqs)\n",
    "val_dataset = TweeterDataset(val_seqs)\n",
    "test_dataset = TweeterDataset(test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f72ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import collate_fn\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"data\"][\"train_batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"data\"][\"val_batch_size\"],\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config[\"data\"][\"test_batch_size\"],\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e84e96ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlstm_train\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training, validation, evaluate_rouge_on_val, generate\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer\u001b[38;5;241m.\u001b[39mfrom_file(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/RNN-practice/src/lstm_train.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoCompleteLSTM()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\u001b[39;00m\n",
      "File \u001b[0;32m~/RNN-practice/.venv/lib/python3.10/site-packages/torch/optim/adam.py:58\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lr\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor lr must be 1-element\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m eps:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "from src.lstm_train import training, validation, evaluate_rouge_on_val, generate\n",
    "tokenizer = Tokenizer.from_file(config[\"data\"][\"tokenizer_file_name\"])\n",
    "\n",
    "num_epochs = config[\"training\"][\"epochs\"]\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"–≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    training(train_dataloader)\n",
    "    should_stop = validation(val_dataloader)\n",
    "    if should_stop: break\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        evaluate_rouge_on_val(test_dataloader, tokenizer)\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "        generate(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8755a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76/76 [00:00<00:00, 652.89it/s, Materializing param=transformer.wte.weight]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:\n",
      "1) I am about to give you a big thank you.‚Äù\n",
      "2) What is going on with my life, I really do have to be able to take care of the body, that's a\n",
      "3) Don't you mind if you don't like the idea?      The end result is: So\n",
      "4) Let us deal with it with the same problem as a school student.   But in a world where our world\n",
      "5) Deep Learning is a tool to learn and develop complex programming languages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/ubuntu/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/6e5315f72865c2eaa764c8361360bb938740b9c120a2cf3a7ad218aa0ce452ed (last modified on Tue Feb 17 14:28:43 2026) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'absl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m transformer_generate(generator, generation_prompts)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrouge_calc_for_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RNN-practice/src/eval_transformer_pipeline.py:33\u001b[0m, in \u001b[0;36mrouge_calc_for_transformer\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrouge_calc_for_transformer\u001b[39m(generator):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ ROUGE\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(processed_path):\n\u001b[0;32m---> 33\u001b[0m         rouge \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrouge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(processed_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     36\u001b[0m             lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "File \u001b[0;32m~/RNN-practice/.venv/lib/python3.10/site-packages/evaluate/loading.py:751\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[1;32m    748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m evaluation_module_factory(\n\u001b[1;32m    749\u001b[0m     path, module_type\u001b[38;5;241m=\u001b[39mmodule_type, revision\u001b[38;5;241m=\u001b[39mrevision, download_config\u001b[38;5;241m=\u001b[39mdownload_config, download_mode\u001b[38;5;241m=\u001b[39mdownload_mode\n\u001b[1;32m    750\u001b[0m )\n\u001b[0;32m--> 751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m \u001b[43mimport_main_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_type \u001b[38;5;129;01mand\u001b[39;00m module_type \u001b[38;5;241m!=\u001b[39m evaluation_instance\u001b[38;5;241m.\u001b[39mmodule_type:\n",
      "File \u001b[0;32m~/RNN-practice/.venv/lib/python3.10/site-packages/evaluate/loading.py:76\u001b[0m, in \u001b[0;36mimport_main_class\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimport_main_class\u001b[39m(module_path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[Type[DatasetBuilder], Type[EvaluationModule]]]:\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Import a module at module_path and return its main class, a Metric by default\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     main_cls_type \u001b[38;5;241m=\u001b[39m EvaluationModule\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Find the main class in our imported module\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/6e5315f72865c2eaa764c8361360bb938740b9c120a2cf3a7ad218aa0ce452ed/rouge.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\" ROUGE metric from Google Research github repo. \"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# The dependencies in https://github.com/google-research/google-research/blob/master/rouge/requirements.txt\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mabsl\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'absl'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from src.eval_transformer_pipeline import transformer_generate\n",
    "from src.eval_transformer_pipeline import rouge_calc_for_transformer\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\n",
    "generation_prompts = [\"I am about\", \"What is going on with\", \"Don't you mind if\", \"Let us deal with\", \"Deep Learning is\"]\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "transformer_generate(generator, generation_prompts)\n",
    "rouge_calc_for_transformer(generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
